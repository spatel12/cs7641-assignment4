{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Value-Itertion.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "Citation: https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gym import wrappers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 2000):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return scores\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iterations = 10000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 1e-06\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  3.3333366666733304e-118\n",
      "gamma = 1e-05\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  1.0000033333333351e-118\n",
      "gamma = 0.0001\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  6.667000000006676e-88\n",
      "gamma = 0.001\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  6.670013346673357e-82\n",
      "gamma = 0.01\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  3.3333400006667017e-34\n",
      "gamma = 0.1\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  4.787448426022346e-24\n",
      "gamma = 0.2\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  1.0552169000220018e-17\n",
      "gamma = 0.4\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  1.516071285389632e-11\n",
      "gamma = 0.8\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  6.571843337414203e-05\n",
      "gamma = 1.0\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  0.8703333333333333\n"
     ]
    }
   ],
   "source": [
    "for i in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.4, 0.8, 1.0]:\n",
    "    print(\"gamma = \" + str(i))\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    gamma = i\n",
    "    env = gym.make(env_name)\n",
    "    optimal_v = value_iteration(env, i);\n",
    "    policy = extract_policy(optimal_v, i)\n",
    "    policy_scores = evaluate_policy(env, policy, i, n=3000)\n",
    "    print('Policy average score = ', np.mean(policy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(policy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 2357.\n",
      "Average time: 2.6078240871429443\n",
      "Average score =  0.878\n"
     ]
    }
   ],
   "source": [
    "env_name  = 'FrozenLake8x8-v0'\n",
    "env = gym.make(env_name)\n",
    "start = time.time()\n",
    "optimal_v = value_iteration(env, gamma);\n",
    "print(\"Average time: \" + str(((time.time() - start))))\n",
    "policy = extract_policy(optimal_v, gamma)\n",
    "policy_scores = evaluate_policy(env, policy, gamma, n=3000)\n",
    "print('Average score = ', np.mean(policy_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Inconsistent shape between the condition and the input (got (10000, 1) and (10000,))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-6558df40208b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_scores\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"YlGnBu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.8/site-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.8/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[1;32m    544\u001b[0m     \u001b[0;31m# Initialize the plotter object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n\u001b[0m\u001b[1;32m    546\u001b[0m                           \u001b[0mannot_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                           yticklabels, mask)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.8/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_matrix_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_where\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Get good names for the rows and columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.8/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36mmasked_where\u001b[0;34m(condition, a, copy)\u001b[0m\n\u001b[1;32m   1939\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mcshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mashape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcshape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mashape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1941\u001b[0;31m         raise IndexError(\"Inconsistent shape between the condition and the input\"\n\u001b[0m\u001b[1;32m   1942\u001b[0m                          \" (got %s and %s)\" % (cshape, ashape))\n\u001b[1;32m   1943\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Inconsistent shape between the condition and the input (got (10000, 1) and (10000,))"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 16))\n",
    "sns.heatmap(policy,  cmap=\"YlGnBu\", annot=True, cbar=False, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward:  1.0\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/this-is-how-reinforcement-learning-works-5080b3a335d6\n",
    "# function for displaying a heatmap\n",
    "def display_value_iteration(P, env = gym.make('FrozenLake8x8-v0')):\n",
    "    nb_states = env.observation_space.n\n",
    "    visited_states = np.zeros(nb_states).astype(bool)\n",
    "    visited_states[0] = 1\n",
    "    states_labels = np.where(P==0, '<', \n",
    "                              np.where(P==1, 'v', \n",
    "                                       np.where(P==2, '>', \n",
    "                                                np.where(P==3, '^', P)\n",
    "                                               )\n",
    "                                      )\n",
    "                             ) \n",
    "    desc = env.unwrapped.desc.ravel().astype(str)\n",
    "    colors = np.where(desc=='S','y',np.where(desc=='F','b',np.where(desc=='H','r',np.where(desc=='G','g',desc))))\n",
    "    states_labels = np.zeros(nb_states).astype(str)\n",
    "    states_labels[:] = ''\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    #env.render()\n",
    "    done = False\n",
    "    while done != True: \n",
    "        best_a = P[s] # select the best next action from the policy\n",
    "        states_labels[s] = '^' if best_a==0 else ('>' if best_a==1 else ('v' if best_a==2 else '<'))   \n",
    "        #print(s, best_a)\n",
    "        s, rew, done, info = env.step(best_a) #take step using selected action\n",
    "        total_reward = total_reward + rew\n",
    "        visited_states[s] = 1 # mark the state as visited\n",
    "        #env.render()\n",
    "    ax = sns.heatmap(P.reshape(int(np.sqrt(nb_states)),int(np.sqrt(nb_states))), \n",
    "                 linewidth=0.5, \n",
    "                 annot=states_labels.reshape(int(np.sqrt(nb_states)),int(np.sqrt(nb_states))), \n",
    "                 cmap=list(colors),\n",
    "                 fmt = '',\n",
    "                 cbar=False)\n",
    "    plt.show()\n",
    "    print(\"Total Reward: \", total_reward)\n",
    "    \n",
    "# display heatmap for a 4x4 board\n",
    "display_value_iteration(policy, env = env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many iterations does it take to converge?\n",
    "# 2357\n",
    "\n",
    "# How did you choose to define convergence?\n",
    "# if the sum of the differences between the values of each iteration is less than epsilon, then we can say that the model has converged\n",
    "\n",
    "# episilon = 1e-20, which means that there is a low chance of selecting a random action instead of the desired action\n",
    "# The reward for falling into the hole ends the game\n",
    "\n",
    "# How did the number of states affect things, if at all?\n",
    "# 4x4=16 states vs 8x8=64 states\n",
    "# Clock Time: 0.000736 seconds, 0.00233 seconds\n",
    "# Policy average score: 0.737, 0.866\n",
    "# Iterations to value iteration convergence: 1373, 2357\n",
    "# Total Reward: 1.0, 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-5a8b2b21f4af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Generate random policy with equal probabilities of each action given any state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Evaluate the policy to get state values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Plot heatmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-5a8b2b21f4af>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[0;34m(env, policy, gamma, theta)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Given each state, we've 4 actions associated with different probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# 0.25 x 4 in this case, so we'll be looping 4 times (4 action probabilities) at each state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_prob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;31m# 2.1.1 Loop through to get transition probabilities, next state, rewards and whether the game ended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "S_n=40\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=1., theta=1e-8):\n",
    "    r\"\"\"Policy evaluation function. Loop until state values stable, delta < theta.\n",
    "\n",
    "    Returns V comprising values of states under given policy.\n",
    "\n",
    "    Args:\n",
    "        env (gym.env): OpenAI environment class instantiated and assigned to an object.\n",
    "        policy (np.array): policy array to evaluate\n",
    "        gamma (float): discount rate for rewards\n",
    "        theta (float): tiny positive number, anything below it indicates value function convergence\n",
    "    \"\"\"\n",
    "    # 1. Create state-value array (16,)\n",
    "    V = np.zeros(S_n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        # 2. Loop through states\n",
    "        for s in range(S_n):\n",
    "            Vs = 0\n",
    "\n",
    "            # 2.1 Loop through actions for the unique state\n",
    "            # Given each state, we've 4 actions associated with different probabilities\n",
    "            # 0.25 x 4 in this case, so we'll be looping 4 times (4 action probabilities) at each state\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # 2.1.1 Loop through to get transition probabilities, next state, rewards and whether the game ended\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # State-value function to get our values of states given policy\n",
    "                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n",
    "\n",
    "            # This simple equation allows us to stop this loop when we've converged\n",
    "            # How do we know? The new value of the state is smaller than a tiny positive value we set\n",
    "            # State value change is tiny compared to what we have so we just stop!\n",
    "            delta = max(delta, np.abs(V[s]-Vs))\n",
    "\n",
    "            # 2.2 Update our state value for that state\n",
    "            V[s] = Vs\n",
    "\n",
    "        # 3. Stop policy evaluation if our state values changes are smaller than our tiny positive number\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "# Generate random policy with equal probabilities of each action given any state\n",
    "# Evaluate the policy to get state values\n",
    "V = policy_evaluation(env, policy)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(V.reshape(4, 4),  cmap=\"YlGnBu\", annot=True, cbar=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Reward')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEWCAYAAACKSkfIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdRUlEQVR4nO3debhcVZ3u8e9LEiYJBEzkkhAIYFQCauBG4F7FxoZmUgm2gqBIQNqgDaJCtzLYlzSIoDbYzdPMTR4CMoNIgChGpgAyBYGQQeQQhiQEEghDIIgEfvePtQp2Dmeoc06dOhzW+3meek7V2tNau3a9tWrtqn0UEZiZWRlW6+sKmJlZ8zj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK8r4NfUlnS/q3Bq1rE0mvSBqQH98q6Z8ase68vt9KmtCo9XVhuz+R9JykZ5q97UbpyXMh6U5J2zS6Tu9Vkg6SdEdf16Mj/aGO3SVpR0mPNGA9a0j6s6Rh3Vm+X4a+pCckvSZpuaQXJf1R0rclvd2eiPh2RJxY57p26WieiHgqItaJiDcbUPdJkn7Vav17RMSUnq67i/XYBDgKGBMR/6uN6TtJeiu/2S2X9Iikg5tZx94k6YvA8oh4ID+eJOmN3N7a7Yd9XM02tXUMvVdIGiFppaQt2ph2jaT/6KN6jZIUkgbmxxdI+kkvbzMkfbj2OCJuj4iP9nS9EfE6MBk4ujvL98vQz74YEYOBTYFTgB8B5zd6I7WD5H1oE+D5iFjSwTxPR8Q6wLrAD4DzJPX4oO0OJY08Xr8NXNSq7PL85l67/byNegxoYB3edyJiEXAT8I1quaQNgD2BpnZuest7IBcuASZIWqOrC/bn0AcgIl6KiKnAV0k7YWtY9Z1c0lBJ1+dPBcsk3S5pNUkXkcLvulrPrtIjOETSU8DNrXsJ2RaS7pX0sqRr80Fd6yEvrNax9mlC0u7AscBX8/YeytPfHqLI9fqxpCclLZF0oaT18rRaPSZIeioPzRzX3r6RtF5efmle34/z+ncBpgPDcz0u6GQfR0RMA5YBn6jU82hJj0l6XtIVlX0wRdJR+f6IXOfD8uMt8nOwmqT18/OyVNIL+f7GlfrfKukkSXcCK4DNJf2D0kfblyT9N6DK/B+WdFue9pyky9vZL6sDfw/c1lG787wXSDpL0jRJrwKfk7RlrtuLkuZI2ivPW9uftdsKSVFZ1zclzcttvVHSppVpofRp9dG83jMkqY0qdVbf2nOyXNJcSV/qYN5fSLojHyfrSTpf0mJJi5SG/rr7BjeFVqEP7AfMjYiH661jW687tRrO62ifdtDuicDXgR/m5+m6XD5c0tX5eHxc0hGVZSZJukrSryS9DBwkaTtJd+Xna7Gk/87HFpJm5EUfytv4qlplQ3vHUZ52QT4Gbsj76R5VPj1FxELgBWCHztr7LhHR727AE8AubZQ/BXwn378A+Em+fzJwNjAo33YE1Na6gFFAABcCHwDWqpQNzPPcCiwCts7zXA38Kk/bCVjYXn2BSbV5K9NvBf4p3/8m0AJsDqwD/Bq4qFXdzsv1+iTwOrBlO/vpQuBaYHBe9i/AIe3Vs9Wyb08ndQ72At4Ctsll3wPuBjYG1gDOAS6ttOG6fP9rwGOkXnRt2rX5/geBLwNr5zpeCfym1X55CtgKGAgMA5YDX8nP4w+AlZV9dylwXK7vmsBn2mnbVsCrrcre9bxUjqOXgE/n9Q7Oz8+xQO3NYznw0TaWvbiyT8bn5bbMbfkx8MfKvAFcDwwhdUSWAru3U/8265qn7QMMz3X9KvAqsFGedhBwR552HnAjsHaedk1+Dj8AfAi4Fzi0m6/PtfI++0yl7C7g+/XWsdXxPrCd10qH+7RVnVZZF5V8qBzj9wP/Lz+vmwPzgd0q+/wNYO8871rA/yaF7sC8/nm1Nlae0w+385oa1NFxlOv3PLBdXv/FwGWt2jQVOKKrz0+/7+m38jSwQRvlbwAbAZtGxBuRxtY6u+jQpIh4NSJea2f6RRExOyJeBf4N2LcHPaOqrwOnRcT8iHgFOAbYT6t+yvj3iHgtIh4CHiKF/ypyXfYDjomI5RHxBHAq7+6BdWS4pBeB10ihcGTkMXDS8MhxEbEw0hjjJOAruZ63AZ9RGo75LPBzUmgC/F2eTkQ8HxFXR8SKiFgOnJSnV10QEXMiYiWwBzAnIq6KiDeA/wSqJ6HfIA33DY+Iv0ZEeycEh5BeYK3tm3tdtdvwXH5tRNwZEW8BY0lvxqdExN8i4mZSWO9fXZGkHwEfI73J1fbXyRExL7flp8DYVj3TUyLixYh4Crglb6tLIuLKiHg6It6KiMuBR0nBUTOI9Oa4AWmIdIWkDUlDL9/Px/wS4Jek46fL8mvmSuBAAEmjSQF5SZ11rFc9+7RenwKGRcQJ+XmdT3pjrO6DuyLiN7ner0XE/RFxd0SszK+vc3j38dueHej8OLomIu7NbbuYdx8Py0nHcpe830J/BGkIorVfkN5Vfy9pvqR6ToAs6ML0J0kvpqF11bJjw/P6quseCGxYKasG3QrSwdPa0Fyn1usa0YW6PB0RQ0hj+qeTeiM1mwLX1AKS1Mt5E9gwIh4j9d7Gkj5VXQ88rXQ+4O3Ql7S2pHOUhp5eBmYAQ1q9eVb38/Dq4/zGXZ3+Q9Jwz7354/I3adsLpB57a1dExJDK7en26pDfAGpW2a+S9iB9Etq70mnYFPivyv5alutafT7qeV47JOlASQ9WtrM1qx6XHyb1kP89Iv5WqdsgYHFluXNIPf62tlEdwtqknapMAfaRtCapo3FjfjOpp471qmefdmVdw6tv+qReePV1t0omSPqI0pDkM/n4/WkX2tHpcUTnx8Ng4MU6t/e2903oS/oUaYe9q3eXe7pHRcTmpGGKIyXtXJvczio7+yQwsnJ/E1Iv8zlS2K1dqdcA0rBEvet9mnQAVte9Eni2k+Vae453er7VdS3q4nrIPfkfAR+XtHcuXgDs0Sok14x0Ig9SsH8FWD2X3QZMANYHHszzHAV8FNg+ItYlfSqAyjg9q+6vxVT2ex7zfvtxRDwTEd+KiOHAocCZqnx7oqIlL15vOFTr8DQwUqueVH57v+Y3tinAvhFRDYkFpOGS6v5aKyL+WGcdOpV7uOcBhwMfzG/Ys1l1f84DDgZ+q3dOyi8gDRMOrdRt3YjYqq3txKonu59qpzp3kEJ4PHAA+QRunXWseTX/XbtSVv2mWU/2aevX4QLg8VbrGhwRe3awzFnAn4HR+fg9tp12tKXD46hOW5I+6XdJvw99SetK+gJwGWmc8+E25vmC0kk+kcYa3ySNT0MK0827sekDJI2RtDZwAnBVpK90/gVYU9LnJQ0ijTNWz7A/C4xS+99EuRT4gaTNJK1D6j1cnj/i1S3X5QrgJEmD84vtSKBbX/XLvcJTSWOekM6RnFT7KC1pmKTxlUVuI72waye0bs2P74h3vvo6mDR09KLSSeDjO6nGDcBWkv4xDyMdQSUEJO2jd04Ev0B6kb7VeiW5LX+g/o/iVfeQel0/lDRI0k7AF4HLJK1LOodyXBtDS2cDx0jaKtd1PUn7dGP7NatJWrNyW4M0Hh+k8wEofcV269YLRsSlpID6g6QtImIx8Hvg1Px6Wk3phHt39k9tG7XzYj8jDUFclyfVVce8jqWkEDxA0oD8ya36VdCe7NPWr/t7geWSfiRprby9rXNnsj2DgZeBVyR9DPhOJ9uoavc4qqfyucOyAem8Wpf059C/TtJy0jv0ccBppB5MW0aTXuSvkE4onRkRt+RpJwM/zh/p/qUL27+IdLLlGdJJwyMgfZsI+Gfgf0gH7KtA9ds8V+a/z0v6UxvrnZzXPQN4HPgr8N0u1Kvqu3n780k9r0vy+rtrMrCJ0nfc/4t0Iun3+Xm4G9i+Mu9tpBdFLfTvIPXYZlTm+U/SCbHn8vK/62jjEfEc6STgKaSTXKOBOyuzfAq4R9IruW7fy2OzbTmHrp3fqNXhb6QX5x653mcCB0bEn4FtSZ9cflkdAsnLXUMKwMvyUMDsvI7u2p/0hlm7PRYRc0lvzHeRAufjrLp/qu2YQuqs3CxpFGn8fXVgLukN8yrSebCeuJDUe708f1qkK3XMvgX8K+n53gp4uxffw316PjAmv+5/kzsiXyANST5Oem7/B1ivg3X8C+mLCstJn15af1tsEjAlb2Pf6oROjqN6fA2YUtuvXVH7BotZcZS+Cnp4vHNy2uw9L3+qewj4bHT8O5u2l3fom5mVoz8P75iZWRc59M3MCuLQNzMrSF9fNKhDQ4cOjVGjRvV1NczM+pX777//uYho89LL7+nQHzVqFDNnzuzrapiZ9SuSnmxvmod3zMwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK8p7+Ra6ZWV8bdfQNfbLdJ075fK+s1z19M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzArSaehLGinpFklzJc2R9L1cPknSIkkP5tuelWWOkdQi6RFJu1XKd89lLZKO7p0mmZlZe+r5nv5K4KiI+JOkwcD9kqbnab+MiP+ozixpDLAfsBUwHPiDpI/kyWcA/wAsBO6TNDUi5jaiIWZm1rlOQz8iFgOL8/3lkuYBIzpYZDxwWUS8DjwuqQXYLk9riYj5AJIuy/M69M3MmqRLY/qSRgHbAPfkosMlzZI0WdL6uWwEsKCy2MJc1l55621MlDRT0sylS5d2pXpmZtaJukNf0jrA1cD3I+Jl4CxgC2As6ZPAqY2oUEScGxHjImLcsGFt/jN3MzPrprquvSNpECnwL46IXwNExLOV6ecB1+eHi4CRlcU3zmV0UG5mZk1Qz7d3BJwPzIuI0yrlG1Vm+xIwO9+fCuwnaQ1JmwGjgXuB+4DRkjaTtDrpZO/UxjTDzMzqUU9P/9PAN4CHJT2Yy44F9pc0FgjgCeBQgIiYI+kK0gnalcBhEfEmgKTDgRuBAcDkiJjTwLaYmVkn6vn2zh2A2pg0rYNlTgJOaqN8WkfLmZlZ7/Ivcs3MCuLQNzMriEPfzKwgDn0zs4I49M3MCuLQNzMriEPfzKwgDn0zs4I49M3MCuLQNzMriEPfzKwgDn0zs4I49M3MCuLQNzMriEPfzKwgDn0zs4I49M3MCuLQNzMriEPfzKwgDn0zs4I49M3MCuLQNzMriEPfzKwgDn0zs4I49M3MCuLQNzMriEPfzKwgDn0zs4I49M3MCuLQNzMrSKehL2mkpFskzZU0R9L3cvkGkqZLejT/XT+XS9LpklokzZK0bWVdE/L8j0qa0HvNMjOzttTT018JHBURY4AdgMMkjQGOBm6KiNHATfkxwB7A6HybCJwF6U0COB7YHtgOOL72RmFmZs3RaehHxOKI+FO+vxyYB4wAxgNT8mxTgL3z/fHAhZHcDQyRtBGwGzA9IpZFxAvAdGD3hrbGzMw61KUxfUmjgG2Ae4ANI2JxnvQMsGG+PwJYUFlsYS5rr9zMzJqk7tCXtA5wNfD9iHi5Oi0iAohGVEjSREkzJc1cunRpI1ZpZmZZXaEvaRAp8C+OiF/n4mfzsA3575JcvggYWVl841zWXvkqIuLciBgXEeOGDRvWlbaYmVkn6vn2joDzgXkRcVpl0lSg9g2cCcC1lfID87d4dgBeysNANwK7Slo/n8DdNZeZmVmTDKxjnk8D3wAelvRgLjsWOAW4QtIhwJPAvnnaNGBPoAVYARwMEBHLJJ0I3JfnOyEiljWkFWZmVpdOQz8i7gDUzuSd25g/gMPaWddkYHJXKmhmZo3jX+SamRXEoW9mVhCHvplZQRz6ZmYFceibmRXEoW9mVhCHvplZQRz6ZmYFceibmRXEoW9mVhCHvplZQRz6ZmYFceibmRXEoW9mVhCHvplZQRz6ZmYFceibmRXEoW9mVhCHvplZQRz6ZmYFceibmRXEoW9mVhCHvplZQRz6ZmYFceibmRXEoW9mVhCHvplZQRz6ZmYFceibmRXEoW9mVhCHvplZQToNfUmTJS2RNLtSNknSIkkP5tuelWnHSGqR9Iik3Srlu+eyFklHN74pZmbWmXp6+hcAu7dR/suIGJtv0wAkjQH2A7bKy5wpaYCkAcAZwB7AGGD/PK+ZmTXRwM5miIgZkkbVub7xwGUR8TrwuKQWYLs8rSUi5gNIuizPO7fLNTYzs27ryZj+4ZJm5eGf9XPZCGBBZZ6Fuay98neRNFHSTEkzly5d2oPqmZlZa90N/bOALYCxwGLg1EZVKCLOjYhxETFu2LBhjVqtmZlRx/BOWyLi2dp9SecB1+eHi4CRlVk3zmV0UG5mZk3SrZ6+pI0qD78E1L7ZMxXYT9IakjYDRgP3AvcBoyVtJml10sneqd2vtpmZdUenPX1JlwI7AUMlLQSOB3aSNBYI4AngUICImCPpCtIJ2pXAYRHxZl7P4cCNwABgckTMaXhrzMysQ/V8e2f/NorP72D+k4CT2iifBkzrUu3MzKyh/ItcM7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzArSaehLmixpiaTZlbINJE2X9Gj+u34ul6TTJbVImiVp28oyE/L8j0qa0DvNMTOzjtTT078A2L1V2dHATRExGrgpPwbYAxidbxOBsyC9SQDHA9sD2wHH194ozMyseToN/YiYASxrVTwemJLvTwH2rpRfGMndwBBJGwG7AdMjYllEvABM591vJGZm1su6O6a/YUQszvefATbM90cACyrzLcxl7ZW/i6SJkmZKmrl06dJuVs/MzNrS4xO5ERFANKAutfWdGxHjImLcsGHDGrVaMzOj+6H/bB62If9dkssXASMr822cy9orNzOzJupu6E8Fat/AmQBcWyk/MH+LZwfgpTwMdCOwq6T18wncXXOZmZk10cDOZpB0KbATMFTSQtK3cE4BrpB0CPAksG+efRqwJ9ACrAAOBoiIZZJOBO7L850QEa1PDpuZWS/rNPQjYv92Ju3cxrwBHNbOeiYDk7tUOzMzayj/ItfMrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgvQo9CU9IelhSQ9KmpnLNpA0XdKj+e/6uVySTpfUImmWpG0b0QAzM6tfI3r6n4uIsRExLj8+GrgpIkYDN+XHAHsAo/NtInBWA7ZtZmZd0BvDO+OBKfn+FGDvSvmFkdwNDJG0US9s38zM2tHT0A/g95LulzQxl20YEYvz/WeADfP9EcCCyrILc9kqJE2UNFPSzKVLl/awemZmVjWwh8t/JiIWSfoQMF3Sn6sTIyIkRVdWGBHnAucCjBs3rkvLmplZx3rU04+IRfnvEuAaYDvg2dqwTf67JM++CBhZWXzjXGZmZk3S7dCX9AFJg2v3gV2B2cBUYEKebQJwbb4/FTgwf4tnB+ClyjCQmZk1QU+GdzYErpFUW88lEfE7SfcBV0g6BHgS2DfPPw3YE2gBVgAH92DbZmbWDd0O/YiYD3yyjfLngZ3bKA/gsO5uz8zMes6/yDUzK4hD38ysIA59M7OCOPTNzAri0DczK0hPf5H7njbq6Bv6ZLtPnPL5PtmumVln3NM3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK4tA3MytI00Nf0u6SHpHUIunoZm/fzKxkTQ19SQOAM4A9gDHA/pLGNLMOZmYla3ZPfzugJSLmR8TfgMuA8U2ug5lZsQY2eXsjgAWVxwuB7aszSJoITMwPX5H0SA+2NxR4rgfLd4t+1uwtrqJP2tyHSmsvuM1F0M961OZN25vQ7NDvVEScC5zbiHVJmhkR4xqxrv6itDaX1l5wm0vRW21u9vDOImBk5fHGuczMzJqg2aF/HzBa0maSVgf2A6Y2uQ5mZsVq6vBORKyUdDhwIzAAmBwRc3pxkw0ZJupnSmtzae0Ft7kUvdJmRURvrNfMzN6D/ItcM7OCOPTNzArS70O/s8s6SFpD0uV5+j2SRjW/lo1VR5uPlDRX0ixJN0lq9zu7/UW9l++Q9GVJIanff72vnjZL2jc/13MkXdLsOjZaHcf2JpJukfRAPr737It6NoqkyZKWSJrdznRJOj3vj1mStu3xRiOi395IJ4MfAzYHVgceAsa0muefgbPz/f2Ay/u63k1o8+eAtfP975TQ5jzfYGAGcDcwrq/r3YTneTTwALB+fvyhvq53E9p8LvCdfH8M8ERf17uHbf4ssC0wu53pewK/BQTsANzT0232955+PZd1GA9MyfevAnaWpCbWsdE6bXNE3BIRK/LDu0m/h+jP6r18x4nAz4C/NrNyvaSeNn8LOCMiXgCIiCVNrmOj1dPmANbN99cDnm5i/RouImYAyzqYZTxwYSR3A0MkbdSTbfb30G/rsg4j2psnIlYCLwEfbErtekc9ba46hNRT6M86bXP+2DsyIm5oZsV6UT3P80eAj0i6U9LdknZvWu16Rz1tngQcIGkhMA34bnOq1me6+nrv1HvuMgzWOJIOAMYBf9fXdelNklYDTgMO6uOqNNtA0hDPTqRPczMkfTwiXuzTWvWu/YELIuJUSf8HuEjS1hHxVl9XrL/o7z39ei7r8PY8kgaSPhI+35Ta9Y66LmUhaRfgOGCviHi9SXXrLZ21eTCwNXCrpCdIY59T+/nJ3Hqe54XA1Ih4IyIeB/5CehPor+pp8yHAFQARcRewJulibO9XDb90TX8P/Xou6zAVmJDvfwW4OfIZkn6q0zZL2gY4hxT4/X2cFzppc0S8FBFDI2JURIwincfYKyJm9k11G6KeY/s3pF4+koaShnvmN7OSDVZPm58CdgaQtCUp9Jc2tZbNNRU4MH+LZwfgpYhY3JMV9uvhnWjnsg6STgBmRsRU4HzSR8AW0gmT/fquxj1XZ5t/AawDXJnPWT8VEXv1WaV7qM42v6/U2eYbgV0lzQXeBP41Ivrtp9g623wUcJ6kH5BO6h7Unztxki4lvXEPzecpjgcGAUTE2aTzFnsCLcAK4OAeb7Mf7y8zM+ui/j68Y2ZmXeDQNzMriEPfzKwgDn0zs4I49M3MCuLQt+JIelPSg5JmS7pO0pA+qset/fwHZNYPOfStRK9FxNiI2Jr0243DenuD+dfgZn3OoW+lu4t8AStJW0j6naT7Jd0u6WOSBkh6PP8ickj+lPDZPP8MSaMlbSfprnyN9z9K+miefpCkqZJuBm6StJakyyTNk3QNsFaftdqK5d6HFUvSANJP+s/PRecC346IRyVtD5wZEX8v6RHStds3A/4E7CjpHtJVPR+VtC6wY/5F6S7AT4Ev53VuC3wiIpZJOhJYERFbSvpEXpdZUzn0rURrSXqQ1MOfB0yXtA7wf3nn0hUAa+S/t5P+2cVmwMmk69jfRrpWDKSL+E2RNJp0aYBBlW1Nj4ja9dI/C5wOEBGzJM3qhbaZdcjDO1ai1yJiLLAp6T8SHUZ6LbyYx/prty3z/DOAHUn/5GMaMIR0vZTb8/QTgVvyOYIvki4CVvNqbzfGrCsc+las/N/FjiBdxGsF8LikfeDt/036yTzrvaRPAW9FxF+BB4FDSW8GkHr6tcvdHtTBJmcAX8vr3xr4RMMaY1Ynh74VLSIeAGaR/jnH14FDJD0EzCH/q778/wgWkC7ZDKmHPxh4OD/+OXCypAfoeMj0LGAdSfOAE4D7G9sas875KptmZgVxT9/MrCAOfTOzgjj0zcwK4tA3MyuIQ9/MrCAOfTOzgjj0zcwK8v8BB0hzNbNOmk4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(policy_scores)\n",
    "plt.title(\"Distribution of Rewards (Frozen Lake - Value Iteration)\")\n",
    "plt.xlabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(policy_scores, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
